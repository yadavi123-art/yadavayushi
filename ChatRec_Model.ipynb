{"cells": [{"cell_type": "code", "metadata": {}, "source": "# Offline Chat-Reply Recommendation (GPT-2)\n# - Loads processed context->reply pairs\n# - Fine-tunes GPT-2 offline\n# - Evaluates BLEU, ROUGE-L, Perplexity\n# - Saves model + tokenizer + Model.joblib\n\nimport os, math, random, json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport nltk\n\n# Ensure NLTK data (punkt) is available\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "DATA_PATH = Path(\"/mnt/data/offline_chatbot/processed_conversations.csv\")\nassert DATA_PATH.exists(), f\"Processed pairs not found at {DATA_PATH}\"\n\ndf = pd.read_csv(DATA_PATH)\nprint(\"Total pairs:\", len(df))\ndf.head()\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\nlen(train_df), len(val_df)\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "MODEL_NAME = \"gpt2\"  # must be preloaded offline in the environment\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nspecial_tokens = {\"additional_special_tokens\": [\"<BOS>\", \"<EOS>\", \"<SEP>\", \"<USER_A>\", \"<USER_B>\"]}\nnum_added = tokenizer.add_special_tokens(special_tokens)\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\nif num_added > 0:\n    model.resize_token_embeddings(len(tokenizer))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef build_prompt(context: str):\n    # Convert role tags to special tokens to help the model\n    # Input example:\n    # User B: ...\n    # User A: ...\n    ctx = context.replace(\"User A:\", \"<USER_A>:\").replace(\"User B:\", \"<USER_B>:\")\n    return f\"<BOS> {ctx}\n<USER_A>: \"  # we want the model to complete A's reply\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "class ChatDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_length=512):\n        self.df = df.reset_index(drop=True)\n        self.tok = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        prompt = build_prompt(row['context'])\n        reply = row['reply'] + \" <EOS>\"\n        # Full text is prompt + reply\n        full_text = prompt + reply\n\n        enc = self.tok(\n            full_text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        input_ids = enc.input_ids[0]\n        attention_mask = enc.attention_mask[0]\n\n        # Build labels: ignore context tokens\n        prompt_enc = self.tok(\n            prompt,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        prompt_len = int(prompt_enc.attention_mask[0].sum().item())\n\n        labels = input_ids.clone()\n        # mask out loss for context\n        labels[:prompt_len-0] = -100  # ignore context positions\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\ntrain_ds = ChatDataset(train_df, tokenizer)\nval_ds = ChatDataset(val_df, tokenizer)\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "OUT_DIR = Path(\"./gpt2_offline_chatbot\")\nOUT_DIR.mkdir(exist_ok=True, parents=True)\n\ntraining_args = TrainingArguments(\n    output_dir=str(OUT_DIR / \"checkpoints\"),\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=2,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=0,\n    logging_steps=10,\n    save_steps=200,\n    evaluation_strategy=\"epoch\",\n    fp16=torch.cuda.is_available(),\n    report_to=[],\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=data_collator,\n)\n\ntrainer.train()\neval_metrics = trainer.evaluate()\n\neval_loss = eval_metrics.get('eval_loss', None)\nperplexity = math.exp(eval_loss) if eval_loss is not None else None\nprint(\"Eval loss:\", eval_loss, \"Perplexity:\", perplexity)\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "@torch.no_grad()\ndef generate_reply(context, max_new_tokens=64, temperature=0.7, top_p=0.9):\n    prompt = build_prompt(context)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=top_p,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    gen_text = tokenizer.decode(out[0], skip_special_tokens=True)\n    # Extract only the part after the last \"<USER_A>:\" as the reply\n    if \"<USER_A>:\" in gen_text:\n        reply = gen_text.split(\"<USER_A>:\")[-1].strip()\n    else:\n        reply = gen_text\n    return reply\n\ndef rouge_l_score(ref, hyp):\n    # Simple ROUGE-L based on LCS length\n    ref_tokens = ref.split()\n    hyp_tokens = hyp.split()\n    # LCS dynamic programming\n    n, m = len(ref_tokens), len(hyp_tokens)\n    dp = [[0]*(m+1) for _ in range(n+1)]\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if ref_tokens[i-1] == hyp_tokens[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n    lcs = dp[n][m]\n    prec = lcs / max(1, m)\n    rec = lcs / max(1, n)\n    if prec + rec == 0:\n        return 0.0\n    beta = (rec / (prec + 1e-8))\n    fscore = (1 + beta**2) * (prec * rec) / (rec**2 + prec + 1e-8)\n    return fscore\n\n# Evaluate on validation set\nsmooth = SmoothingFunction().method1\nbleus, rouges = [], []\nsamples = []\n\nfor _, row in val_df.iterrows():\n    ctx, ref = row['context'], row['reply']\n    hyp = generate_reply(ctx)\n    # BLEU-1..4 using NLTK (default weights emphasize 4-gram; for short replies we use BLEU-2-ish smoothing)\n    bleu = sentence_bleu([ref.split()], hyp.split(), smoothing_function=smooth, weights=(0.5, 0.5, 0, 0))\n    rouge = rouge_l_score(ref, hyp)\n    bleus.append(bleu)\n    rouges.append(rouge)\n    samples.append({\"context\": ctx, \"ref\": ref, \"hyp\": hyp})\n\nmetrics = {\n    \"val_bleu_mean\": float(np.mean(bleus) if bleus else 0.0),\n    \"val_rougeL_mean\": float(np.mean(rouges) if rouges else 0.0),\n    \"val_perplexity\": float(perplexity) if perplexity is not None else None\n}\nmetrics\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "from joblib import dump\n\nSAVE_DIR = OUT_DIR / \"artifact\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\nmodel.save_pretrained(SAVE_DIR.as_posix())\ntokenizer.save_pretrained(SAVE_DIR.as_posix())\n\nwith open(SAVE_DIR / \"metrics.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nwith open(SAVE_DIR / \"samples.jsonl\", \"w\") as f:\n    for s in samples:\n        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n\ndump({\n    \"model_dir\": SAVE_DIR.as_posix(),\n    \"model_name\": \"gpt2\",\n    \"special_tokens\": [\"<BOS>\", \"<EOS>\", \"<SEP>\", \"<USER_A>\", \"<USER_B>\"],\n    \"notes\": \"Causal LM fine-tuned for next-reply generation from context\"\n}, SAVE_DIR / \"Model.joblib\")\n\nwith open(SAVE_DIR / \"ReadMe.txt\", \"w\") as f:\n    f.write(\n\"\"\"\nOffline Chat-Reply Recommendation (GPT-2)\n\nSteps to run:\n1) Ensure processed_conversations.csv exists (path set at top).\n2) Run all cells to fine-tune GPT-2.\n3) Metrics written to artifact/metrics.json; sample generations in artifact/samples.jsonl.\n4) Exported model/tokenizer under artifact/ (use from_pretrained to reload).\n\nGeneration:\n- Use generate_reply(context_str) to get User A's predicted reply.\n\nNotes:\n- Adjust WINDOW, max_length, epochs, and hyperparameters per dataset size.\n- Perplexity is exp(eval_loss). BLEU/ROUGE-L are approximate indicators for dialogue.\n\"\"\")\nprint(\"Artifacts saved to:\", SAVE_DIR.as_posix())\n", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}